{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 The mathematical minimum \n",
    "\n",
    "Whilst this work is not intended to be a textbook on mathematics, it is worth outlining the core maths essential to understanding quantum information. Beyond the scope of this course, there are a great many applications of the mathematics covered in this textbook. A much richer understanding of the physical world can be understood by applying these topics to the nature of light, itself quantum fundamentally, as well as machine learning and many fields of engineering. \n",
    "\n",
    "Readers with a more rigorous mathematical background may find this material rather basic but may still appreciate the explanations and relationships between the topics. \n",
    "\n",
    "\n",
    "\n",
    "## 3.1 Linear Algebra \n",
    "\n",
    "Like trigonometry, linear algebra underpins most of physics, engineering and computer science. Its importance is difficult to overstate. In 1939, Paul Dirac reformulated quantum mechanics using linear algebra [1]. His contributions to the development of quantum theory are significant and the notation used to describe quantum states with vectors is known as Dirac notation ( to be covered more in chapter 4). There are two key elements to linear algebra: vectors and matrices. \n",
    "\n",
    "\n",
    "### 3.1.1 Vectors\n",
    "\n",
    "Vectors are objects that have both direction & magnitude. Velocity is a vector because it has a magnitude, the speed at which the object travels with, and a direction, where it's moving towards. In linear algebra, a vector can be denoted by putting an arrow over a letter as $ \\ket{v} $ .\n",
    "\n",
    "Imagine a car travelling diagonally across a road. At the same time, the car is travelling along the direction of the road as well as perpendicularly across the road. It can be said that the velocity of the car has a component pointing along the road and another component across the road. \n",
    "\n",
    "What makes vectors useful to work with is the fact that a vector has components. These are the individual magnitudes of the vector in each direction. We can encode the speed of the car along the road ($v_{alon}$)and the speed of the car across the road ($v_{acro}$) in a vector with two components: \n",
    "\n",
    "\n",
    "$$\n",
    "\\ket{v} = \n",
    "\\begin{bmatrix}\n",
    "v_{alon} \\\\ v_{acro}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here the vector $\\ket{v}$ has two components, $v_{alon}$ & $v_{acro}$ so the vector has a dimension of 2. In 3D space, any point can be described by 3 vectors. In general, a vector can be of any dimension, with any number of components. \n",
    "\n",
    "We can write any n-dimensional vector as\n",
    "\n",
    "$$\n",
    "\\ket{v} = \\begin{bmatrix} v_0 \\\\ v_1 \\\\ \\vdots \\\\ v_{n-1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where the subscript $v_i$ indicates the i $^{th}$ component of $\\ket{v}$. Note the convention of starting from 0 and ending at $n-1$. This convention of counting from 0 is widely used in QC and conveniently is also used in python too. \n",
    "\n",
    "### 3.1.2 Adding vectors\n",
    "\n",
    "It can be useful to add vectors. All this requires is adding the corresponding components of each vector. Adding two vectors, $\\ket{a}$ & $\\ket{b}$\n",
    "\n",
    "$$\\ket{a} + \\ket{b} = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_{n-1} \\end{bmatrix} + \\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{n-1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The same can be done in reverse, a vector can be split into different components. For example \n",
    "\n",
    "$$\n",
    "\\ket{a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be useful for expressing a vector in terms of its components. \n",
    "\n",
    "### 3.1.3 Multiplying a vector by a scalar \n",
    "\n",
    "Vectors can be made smaller or bigger by enlarging them by some scale factor. To make a vector bigger by some scale factor, the vector is multiplied by a scalar. To scale the vector $\\ket{v}$ by some scalar quantity, $a$, this would be done by multiplying each component of the vector by the scalar factor. \n",
    "\n",
    "$$\n",
    "a \\ket{v} = a\\begin{bmatrix} v_0 \\\\ v_1 \\\\ \\vdots \\\\ v_{n-1} \\end{bmatrix} = \\begin{bmatrix} a \\times v_0 \\\\ a \\times v_1 \\\\ \\vdots \\\\ a \\times v_{n-1} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "### 3.1.4 The inner product  \n",
    "\n",
    "It can be very useful to compare two vectors by measuring how aligned they are. Imagine two vectors pointing the same way. These two vectors would have a lot in common, and so would be perfectly aligned. The measure of how similar two vectors are is the inner product (sometimes called dot product). The inner product can be calculated by multiplying each component of the vectors and adding them up. \n",
    "\n",
    "For example take the vectors $\\bra{a}$ & $\\ket{b}$ as \n",
    "\n",
    " $$\n",
    " \\ket{a} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} ;  \\ket{b} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} \n",
    " $$\n",
    "\n",
    " The inner product between them is \n",
    "\n",
    " $$\n",
    "\\braket{a|b} = \\begin{bmatrix} 1^* & 2^* \\end{bmatrix}  \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\n",
    " $$\n",
    "\n",
    "Here the 1st component of $\\bra{u}$ is multiplied by the 1st component of $\\ket{v}$ and the second component of of $\\bra{u}$ is multiplied by the second component of $\\ket{v}$.\n",
    "\n",
    "$$\n",
    "= (1 \\times 3) + (2 \\times 1) = 3 + 2 = 5 \n",
    "$$\n",
    "\n",
    "This gives a scalar quantity from two vectors and is used to indicate how alligned they are. The figure below shows this, where the length of $\\ket{a}$ shows the component of $\\ket{u}$ in the direction of $\\ket{v}$. This is then scaled by the length of $\\ket{v}$ to give the inner product . \n",
    "\n",
    "![Dot_product](Images\\Dot_product_visualisation.png)\n",
    "\n",
    "\n",
    "The inner product depends on the angle between the vectors. If the vectors are pointing the same way, it will be bigger, if the vectors are pointing in completely opposite directions, it will be smaller. \n",
    "\n",
    "### 3.1.5 Perpendicular vectors \n",
    "\n",
    "If two vectors have no components in common, their inner product will be zero. This can be shown by two vectors that have an angle of $90\\degree$ between them.  We call these vectors perpendicular. In quantum mechanics, the preferred term is *orthogonal* which means the same thing but applies to more than just vectors. \n",
    "\n",
    "For example the vectors $\\ket{p} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ & $\\ket{q} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ have a inner product  of 0. \n",
    "\n",
    "$$ \\begin{bmatrix} 1^* & 0^* \\end{bmatrix}  \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = (1 \\times 0) + (0 \\times 1) = 0$$\n",
    "\n",
    "\n",
    "\n",
    "### 3.2.1 Matrices\n",
    "\n",
    "Another object, closely related to a vector, is a matrix. A matrix is just an array of numbers which you can perform a matrix product on. All the quantum gates will be represented as matrices, so understanding how they work will be essential for quantum computations. Matrices are usually denoted by capital letters, for example $M$\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$M$ has 4 elements: 1,2,3 & 4. \n",
    "\n",
    "### 3.2.2 Matrix-vector product \n",
    "\n",
    "As well as storing information, matrices can be used to transform vectors. This is done using the matrix-vector product. The matrix $M$ acting on the vector $\\ket{a}$ is denoted  as $M\\ket{a}$. The matrix multiplication is done by multiplying the corresponding elements of $M$ & $\\ket{a}$. Starting from the first row of $M$ we multiply out the elements of $M$ & $\\ket{a}$ starting from the left and add them up. Let's call the transformed vector $\\ket{a'}$. We can compute $\\ket{a'}$ by doing the matrix-vector product of $M$ on $\\ket{a}$ as\n",
    "\n",
    "\n",
    "$$\n",
    "\\ket{a'} = M\\ket{a}\n",
    "$$\n",
    "\n",
    "Explicity calculating this gives us: \n",
    "\n",
    "$$\n",
    "\\ket{a'} =  \\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix} (1 \\times 1) + (2 \\times 2) \\\\ (3 \\times 1) + (4 \\times 2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ket{a'} = \\begin{bmatrix} 5 \\\\ 11 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 3.2.3 Inverse matrivces\n",
    "\n",
    "For many matrix-vector products, we can go back and get the original vector from the transformed vector. To get the original vector, we do the matrix-vector product, but with the transformed vector and the inverse of the matrix. \n",
    "\n",
    "\n",
    "### 3.2.4 Unitary matrices \n",
    "\n",
    "For all quantum gates, an important property they have is that they are unitary. Unitary matrices have an inverse equal to their conjugate transpose. \n",
    "\n",
    "\n",
    "### Chapter 3.1 Summary \n",
    "\n",
    "- Vectors are a collection of numbers stored in a row or column\n",
    "- Vectors can be added to each other or multiplied by a scalar\n",
    "- The inner product of two vectors tells us how aligned they are\n",
    "- Perpendicular (orthogonal) vectors have an inner product of 0\n",
    "- Matrices are arrays of numbers that can act on vectors \n",
    "- Applying a matrix to a vector turns the vector into another vector\n",
    "- Matrices can have an inverse which does the reverse of the matrix \n",
    "- Unitary matrices have an inverse equal to their complex transpose\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
